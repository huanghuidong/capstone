{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from resnet import *\n",
    "from datetime import datetime\n",
    "import time\n",
    "from cifar10_input import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Train(object):\n",
    "    '''\n",
    "    This Object is responsible for all the training and validation process\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # Set up all the placeholders\n",
    "        self.placeholders()\n",
    "\n",
    "\n",
    "    def placeholders(self):\n",
    "        '''\n",
    "        There are five placeholders in total.\n",
    "        image_placeholder and label_placeholder are for train images and labels\n",
    "        vali_image_placeholder and vali_label_placeholder are for validation imgaes and labels\n",
    "        lr_placeholder is for learning rate. Feed in learning rate each time of training\n",
    "        implements learning rate decay easily\n",
    "        '''\n",
    "        self.image_placeholder = tf.placeholder(dtype=tf.float32,\n",
    "                                                shape=[FLAGS.train_batch_size, IMG_HEIGHT,\n",
    "                                                        IMG_WIDTH, IMG_DEPTH])\n",
    "        self.label_placeholder = tf.placeholder(dtype=tf.int32, shape=[FLAGS.train_batch_size])\n",
    "\n",
    "        self.vali_image_placeholder = tf.placeholder(dtype=tf.float32, shape=[FLAGS.validation_batch_size,\n",
    "                                                                IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH])\n",
    "        self.vali_label_placeholder = tf.placeholder(dtype=tf.int32, shape=[FLAGS.validation_batch_size])\n",
    "\n",
    "        self.lr_placeholder = tf.placeholder(dtype=tf.float32, shape=[])\n",
    "\n",
    "\n",
    "\n",
    "    def build_train_validation_graph(self):\n",
    "        '''\n",
    "        This function builds the train graph and validation graph at the same time.\n",
    "        \n",
    "        '''\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        validation_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        # Logits of training data and valiation data come from the same graph. The inference of\n",
    "        # validation data share all the weights with train data. This is implemented by passing\n",
    "        # reuse=True to the variable scopes of train graph\n",
    "        logits = inference(self.image_placeholder, FLAGS.num_residual_blocks, reuse=False)\n",
    "        vali_logits = inference(self.vali_image_placeholder, FLAGS.num_residual_blocks, reuse=True)\n",
    "\n",
    "        # The following codes calculate the train loss, which is consist of the\n",
    "        # softmax cross entropy and the relularization loss\n",
    "        regu_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        loss = self.loss(logits, self.label_placeholder)\n",
    "        self.full_loss = tf.add_n([loss] + regu_losses)\n",
    "\n",
    "        predictions = tf.nn.softmax(logits)\n",
    "        self.train_top1_error = self.top_k_error(predictions, self.label_placeholder, 1)\n",
    "\n",
    "\n",
    "        # Validation loss\n",
    "        self.vali_loss = self.loss(vali_logits, self.vali_label_placeholder)\n",
    "        vali_predictions = tf.nn.softmax(vali_logits)\n",
    "        self.vali_top1_error = self.top_k_error(vali_predictions, self.vali_label_placeholder, 1)\n",
    "\n",
    "        self.train_op, self.train_ema_op = self.train_operation(global_step, self.full_loss,\n",
    "                                                                self.train_top1_error)\n",
    "        self.val_op = self.validation_op(validation_step, self.vali_top1_error, self.vali_loss)\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        '''\n",
    "        This is the main function for training\n",
    "        '''\n",
    "\n",
    "        # For the first step, we are loading all training images and validation images into the\n",
    "        # memory\n",
    "        all_data, all_labels = prepare_train_data(padding_size=FLAGS.padding_size)\n",
    "        vali_data, vali_labels = read_validation_data()\n",
    "\n",
    "        # Build the graph for train and validation\n",
    "        self.build_train_validation_graph()\n",
    "\n",
    "        # Initialize a saver to save checkpoints. Merge all summaries, so we can run all\n",
    "        # summarizing operations by running summary_op. Initialize a new session\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        init = tf.initialize_all_variables()\n",
    "        sess = tf.Session()\n",
    "\n",
    "\n",
    "        # If you want to load from a checkpoint\n",
    "        if FLAGS.is_use_ckpt is True:\n",
    "            saver.restore(sess, FLAGS.ckpt_path)\n",
    "            print ('Restored from checkpoint...')\n",
    "        else:\n",
    "            sess.run(init)\n",
    "\n",
    "        # This summary writer object helps write summaries on tensorboard\n",
    "        summary_writer = tf.summary.FileWriter(train_dir, sess.graph)\n",
    "\n",
    "\n",
    "        # These lists are used to save a csv file at last\n",
    "        step_list = []\n",
    "        train_error_list = []\n",
    "        val_error_list = []\n",
    "\n",
    "        print ('Start training...')\n",
    "        print ('----------------------------')\n",
    "\n",
    "        for step in range(FLAGS.train_steps):\n",
    "\n",
    "            train_batch_data, train_batch_labels = self.generate_augment_train_batch(all_data, all_labels,\n",
    "                                                                        FLAGS.train_batch_size)\n",
    "\n",
    "\n",
    "            validation_batch_data, validation_batch_labels = self.generate_vali_batch(vali_data,\n",
    "                                                           vali_labels, FLAGS.validation_batch_size)\n",
    "\n",
    "            # Want to validate once before training. You may check the theoretical validation\n",
    "            # loss first\n",
    "            if step % FLAGS.report_freq == 0:\n",
    "\n",
    "                if FLAGS.is_full_validation is True:\n",
    "                    validation_loss_value, validation_error_value = self.full_validation(loss=self.vali_loss,\n",
    "                                            top1_error=self.vali_top1_error, vali_data=vali_data,\n",
    "                                            vali_labels=vali_labels, session=sess,\n",
    "                                            batch_data=train_batch_data, batch_label=train_batch_labels)\n",
    "\n",
    "                    vali_summ = tf.Summary()\n",
    "                    vali_summ.value.add(tag='full_validation_error',\n",
    "                                        simple_value=validation_error_value.astype(np.float))\n",
    "                    summary_writer.add_summary(vali_summ, step)\n",
    "                    summary_writer.flush()\n",
    "\n",
    "                else:\n",
    "                    _, validation_error_value, validation_loss_value = sess.run([self.val_op,\n",
    "                                                                     self.vali_top1_error,\n",
    "                                                                 self.vali_loss],\n",
    "                                                {self.image_placeholder: train_batch_data,\n",
    "                                                 self.label_placeholder: train_batch_labels,\n",
    "                                                 self.vali_image_placeholder: validation_batch_data,\n",
    "                                                 self.vali_label_placeholder: validation_batch_labels,\n",
    "                                                 self.lr_placeholder: FLAGS.init_lr})\n",
    "\n",
    "                val_error_list.append(validation_error_value)\n",
    "\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            _, _, train_loss_value, train_error_value = sess.run([self.train_op, self.train_ema_op,\n",
    "                                                           self.full_loss, self.train_top1_error],\n",
    "                                {self.image_placeholder: train_batch_data,\n",
    "                                  self.label_placeholder: train_batch_labels,\n",
    "                                  self.vali_image_placeholder: validation_batch_data,\n",
    "                                  self.vali_label_placeholder: validation_batch_labels,\n",
    "                                  self.lr_placeholder: FLAGS.init_lr})\n",
    "            duration = time.time() - start_time\n",
    "\n",
    "\n",
    "            if step % FLAGS.report_freq == 0:\n",
    "                summary_str = sess.run(summary_op, {self.image_placeholder: train_batch_data,\n",
    "                                                    self.label_placeholder: train_batch_labels,\n",
    "                                                    self.vali_image_placeholder: validation_batch_data,\n",
    "                                                    self.vali_label_placeholder: validation_batch_labels,\n",
    "                                                    self.lr_placeholder: FLAGS.init_lr})\n",
    "                summary_writer.add_summary(summary_str, step)\n",
    "\n",
    "                num_examples_per_step = FLAGS.train_batch_size\n",
    "                examples_per_sec = num_examples_per_step / duration\n",
    "                sec_per_batch = float(duration)\n",
    "\n",
    "                format_str = ('%s: step %d, loss = %.4f (%.1f examples/sec; %.3f ' 'sec/batch)')\n",
    "                print (format_str % (datetime.now(), step, train_loss_value, examples_per_sec,\n",
    "                                    sec_per_batch))\n",
    "                print ('Train top1 error = ', train_error_value)\n",
    "                print ('Validation top1 error = %.4f' % validation_error_value)\n",
    "                print ('Validation loss = ', validation_loss_value)\n",
    "                print ('----------------------------')\n",
    "\n",
    "                step_list.append(step)\n",
    "                train_error_list.append(train_error_value)\n",
    "\n",
    "\n",
    "\n",
    "            if step == FLAGS.decay_step0 or step == FLAGS.decay_step1:\n",
    "                FLAGS.init_lr = 0.1 * FLAGS.init_lr\n",
    "                print ('Learning rate decayed to ', FLAGS.init_lr)\n",
    "\n",
    "            # Save checkpoints every 10000 steps\n",
    "            if step % 10000 == 0 or (step + 1) == FLAGS.train_steps:\n",
    "                checkpoint_path = os.path.join(train_dir, 'model.ckpt')\n",
    "                saver.save(sess, checkpoint_path, global_step=step)\n",
    "\n",
    "                df = pd.DataFrame(data={'step':step_list, 'train_error':train_error_list,\n",
    "                                'validation_error': val_error_list})\n",
    "                df.to_csv(train_dir + FLAGS.version + '_error.csv')\n",
    "\n",
    "\n",
    "    def test(self, test_image_array):\n",
    "        '''\n",
    "        This function is used to evaluate the test data. Please finish pre-precessing in advance\n",
    "\n",
    "        :param test_image_array: 4D numpy array with shape [num_test_images, img_height, img_width,\n",
    "        img_depth]\n",
    "        :return: the softmax probability with shape [num_test_images, num_labels]\n",
    "        '''\n",
    "        num_test_images = len(test_image_array)\n",
    "        num_batches = num_test_images // FLAGS.test_batch_size\n",
    "        remain_images = num_test_images % FLAGS.test_batch_size\n",
    "        print ('%i test batches in total...' %num_batches)\n",
    "\n",
    "        # Create the test image and labels placeholders\n",
    "        self.test_image_placeholder = tf.placeholder(dtype=tf.float32, shape=[FLAGS.test_batch_size,\n",
    "                                                        IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH])\n",
    "\n",
    "        # Build the test graph\n",
    "        logits = inference(self.test_image_placeholder, FLAGS.num_residual_blocks, reuse=False)\n",
    "        predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # Initialize a new session and restore a checkpoint\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "        sess = tf.Session()\n",
    "\n",
    "        saver.restore(sess, FLAGS.test_ckpt_path)\n",
    "        print ('Model restored from ', FLAGS.test_ckpt_path)\n",
    "\n",
    "        prediction_array = np.array([]).reshape(-1, NUM_CLASS)\n",
    "        # Test by batches\n",
    "        for step in range(num_batches):\n",
    "            if step % 10 == 0:\n",
    "                print ('%i batches finished!' %step)\n",
    "            offset = step * FLAGS.test_batch_size\n",
    "            test_image_batch = test_image_array[offset:offset+FLAGS.test_batch_size, ...]\n",
    "\n",
    "            batch_prediction_array = sess.run(predictions,\n",
    "                                        feed_dict={self.test_image_placeholder: test_image_batch})\n",
    "\n",
    "            prediction_array = np.concatenate((prediction_array, batch_prediction_array))\n",
    "\n",
    "        # If test_batch_size is not a divisor of num_test_images\n",
    "        if remain_images != 0:\n",
    "            self.test_image_placeholder = tf.placeholder(dtype=tf.float32, shape=[remain_images,\n",
    "                                                        IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH])\n",
    "            # Build the test graph\n",
    "            logits = inference(self.test_image_placeholder, FLAGS.num_residual_blocks, reuse=True)\n",
    "            predictions = tf.nn.softmax(logits)\n",
    "\n",
    "            test_image_batch = test_image_array[-remain_images:, ...]\n",
    "\n",
    "            batch_prediction_array = sess.run(predictions, feed_dict={\n",
    "                self.test_image_placeholder: test_image_batch})\n",
    "\n",
    "            prediction_array = np.concatenate((prediction_array, batch_prediction_array))\n",
    "\n",
    "        return prediction_array\n",
    "\n",
    "\n",
    "\n",
    "    ## Helper functions\n",
    "    def loss(self, logits, labels):\n",
    "        '''\n",
    "        Calculate the cross entropy loss given logits and true labels\n",
    "        :param logits: 2D tensor with shape [batch_size, num_labels]\n",
    "        :param labels: 1D tensor with shape [batch_size]\n",
    "        :return: loss tensor with shape [1]\n",
    "        '''\n",
    "        labels = tf.cast(labels, tf.int64)\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                                       labels=labels, name='cross_entropy_per_example')\n",
    "        cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "        return cross_entropy_mean\n",
    "\n",
    "\n",
    "    def top_k_error(self, predictions, labels, k):\n",
    "        '''\n",
    "        Calculate the top-k error\n",
    "        :param predictions: 2D tensor with shape [batch_size, num_labels]\n",
    "        :param labels: 1D tensor with shape [batch_size, 1]\n",
    "        :param k: int\n",
    "        :return: tensor with shape [1]\n",
    "        '''\n",
    "        batch_size = predictions.get_shape().as_list()[0]\n",
    "        in_top1 = tf.to_float(tf.nn.in_top_k(predictions, labels, k=1))\n",
    "        num_correct = tf.reduce_sum(in_top1)\n",
    "        return (batch_size - num_correct) / float(batch_size)\n",
    "\n",
    "\n",
    "    def generate_vali_batch(self, vali_data, vali_label, vali_batch_size):\n",
    "        '''\n",
    "        If you want to use a random batch of validation data to validate instead of using the\n",
    "        whole validation data, this function helps you generate that batch\n",
    "        :param vali_data: 4D numpy array\n",
    "        :param vali_label: 1D numpy array\n",
    "        :param vali_batch_size: int\n",
    "        :return: 4D numpy array and 1D numpy array\n",
    "        '''\n",
    "        offset = np.random.choice(10000 - vali_batch_size, 1)[0]\n",
    "        vali_data_batch = vali_data[offset:offset+vali_batch_size, ...]\n",
    "        vali_label_batch = vali_label[offset:offset+vali_batch_size]\n",
    "        return vali_data_batch, vali_label_batch\n",
    "\n",
    "\n",
    "    def generate_augment_train_batch(self, train_data, train_labels, train_batch_size):\n",
    "        '''\n",
    "        This function helps generate a batch of train data, and random crop, horizontally flip\n",
    "        and whiten them at the same time\n",
    "        :param train_data: 4D numpy array\n",
    "        :param train_labels: 1D numpy array\n",
    "        :param train_batch_size: int\n",
    "        :return: augmented train batch data and labels. 4D numpy array and 1D numpy array\n",
    "        '''\n",
    "        offset = np.random.choice(EPOCH_SIZE - train_batch_size, 1)[0]\n",
    "        batch_data = train_data[offset:offset+train_batch_size, ...]\n",
    "        batch_data = random_crop_and_flip(batch_data, padding_size=FLAGS.padding_size)\n",
    "\n",
    "        batch_data = whitening_image(batch_data)\n",
    "        batch_label = train_labels[offset:offset+FLAGS.train_batch_size]\n",
    "\n",
    "        return batch_data, batch_label\n",
    "\n",
    "\n",
    "    def train_operation(self, global_step, total_loss, top1_error):\n",
    "        '''\n",
    "        Defines train operations\n",
    "        :param global_step: tensor variable with shape [1]\n",
    "        :param total_loss: tensor with shape [1]\n",
    "        :param top1_error: tensor with shape [1]\n",
    "        :return: two operations. Running train_op will do optimization once. Running train_ema_op\n",
    "        will generate the moving average of train error and train loss for tensorboard\n",
    "        '''\n",
    "        # Add train_loss, current learning rate and train error into the tensorboard summary ops\n",
    "        tf.summary.scalar('learning_rate', self.lr_placeholder)\n",
    "        tf.summary.scalar('train_loss', total_loss)\n",
    "        tf.summary.scalar('train_top1_error', top1_error)\n",
    "\n",
    "        # The ema object help calculate the moving average of train loss and train error\n",
    "        ema = tf.train.ExponentialMovingAverage(FLAGS.train_ema_decay, global_step)\n",
    "        train_ema_op = ema.apply([total_loss, top1_error])\n",
    "        tf.summary.scalar('train_top1_error_avg', ema.average(top1_error))\n",
    "        tf.summary.scalar('train_loss_avg', ema.average(total_loss))\n",
    "\n",
    "        opt = tf.train.MomentumOptimizer(learning_rate=self.lr_placeholder, momentum=0.9)\n",
    "        train_op = opt.minimize(total_loss, global_step=global_step)\n",
    "        return train_op, train_ema_op\n",
    "\n",
    "\n",
    "    def validation_op(self, validation_step, top1_error, loss):\n",
    "        '''\n",
    "        Defines validation operations\n",
    "        :param validation_step: tensor with shape [1]\n",
    "        :param top1_error: tensor with shape [1]\n",
    "        :param loss: tensor with shape [1]\n",
    "        :return: validation operation\n",
    "        '''\n",
    "\n",
    "        # This ema object help calculate the moving average of validation loss and error\n",
    "\n",
    "        # ema with decay = 0.0 won't average things at all. This returns the original error\n",
    "        ema = tf.train.ExponentialMovingAverage(0.0, validation_step)\n",
    "        ema2 = tf.train.ExponentialMovingAverage(0.95, validation_step)\n",
    "\n",
    "\n",
    "        val_op = tf.group(validation_step.assign_add(1), ema.apply([top1_error, loss]),\n",
    "                          ema2.apply([top1_error, loss]))\n",
    "        top1_error_val = ema.average(top1_error)\n",
    "        top1_error_avg = ema2.average(top1_error)\n",
    "        loss_val = ema.average(loss)\n",
    "        loss_val_avg = ema2.average(loss)\n",
    "\n",
    "        # Summarize these values on tensorboard\n",
    "        tf.summary.scalar('val_top1_error', top1_error_val)\n",
    "        tf.summary.scalar('val_top1_error_avg', top1_error_avg)\n",
    "        tf.summary.scalar('val_loss', loss_val)\n",
    "        tf.summary.scalar('val_loss_avg', loss_val_avg)\n",
    "        return val_op\n",
    "\n",
    "\n",
    "    def full_validation(self, loss, top1_error, session, vali_data, vali_labels, batch_data,\n",
    "                        batch_label):\n",
    "        '''\n",
    "        Runs validation on all the 10000 valdiation images\n",
    "        :param loss: tensor with shape [1]\n",
    "        :param top1_error: tensor with shape [1]\n",
    "        :param session: the current tensorflow session\n",
    "        :param vali_data: 4D numpy array\n",
    "        :param vali_labels: 1D numpy array\n",
    "        :param batch_data: 4D numpy array. training batch to feed dict and fetch the weights\n",
    "        :param batch_label: 1D numpy array. training labels to feed the dict\n",
    "        :return: float, float\n",
    "        '''\n",
    "        num_batches = 10000 // FLAGS.validation_batch_size\n",
    "        order = np.random.choice(10000, num_batches * FLAGS.validation_batch_size)\n",
    "        vali_data_subset = vali_data[order, ...]\n",
    "        vali_labels_subset = vali_labels[order]\n",
    "\n",
    "        loss_list = []\n",
    "        error_list = []\n",
    "\n",
    "        for step in range(num_batches):\n",
    "            offset = step * FLAGS.validation_batch_size\n",
    "            feed_dict = {self.image_placeholder: batch_data, self.label_placeholder: batch_label,\n",
    "                self.vali_image_placeholder: vali_data_subset[offset:offset+FLAGS.validation_batch_size, ...],\n",
    "                self.vali_label_placeholder: vali_labels_subset[offset:offset+FLAGS.validation_batch_size],\n",
    "                self.lr_placeholder: FLAGS.init_lr}\n",
    "            loss_value, top1_error_value = session.run([loss, top1_error], feed_dict=feed_dict)\n",
    "            loss_list.append(loss_value)\n",
    "            error_list.append(top1_error_value)\n",
    "\n",
    "        return np.mean(loss_list), np.mean(error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading images from cifar10_data/cifar-10-batches-py/data_batch_1\n",
      "Reading images from cifar10_data/cifar-10-batches-py/data_batch_2\n",
      "Reading images from cifar10_data/cifar-10-batches-py/data_batch_3\n",
      "Reading images from cifar10_data/cifar-10-batches-py/data_batch_4\n",
      "Reading images from cifar10_data/cifar-10-batches-py/data_batch_5\n",
      "Shuffling\n",
      "Reading images from cifar10_data/cifar-10-batches-py/test_batch\n",
      "Shuffling\n",
      "WARNING:tensorflow:From D:\\resnet\\resnet.py:52: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "WARNING:tensorflow:From C:\\Users\\WENWEN\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:107: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Start training...\n",
      "----------------------------\n",
      "2017-11-22 23:04:57.099616: step 0, loss = 2.5791 (174.3 examples/sec; 0.734 sec/batch)\n",
      "Train top1 error =  0.835938\n",
      "Validation top1 error = 0.9080\n",
      "Validation loss =  2.46162\n",
      "----------------------------\n",
      "2017-11-22 23:05:48.884134: step 391, loss = 1.2172 (1169.9 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.359375\n",
      "Validation top1 error = 0.3520\n",
      "Validation loss =  1.01499\n",
      "----------------------------\n",
      "2017-11-22 23:06:36.883234: step 782, loss = 1.0067 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.296875\n",
      "Validation top1 error = 0.3120\n",
      "Validation loss =  0.78973\n",
      "----------------------------\n",
      "2017-11-22 23:07:24.725168: step 1173, loss = 0.7743 (1170.6 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.21875\n",
      "Validation top1 error = 0.2280\n",
      "Validation loss =  0.660394\n",
      "----------------------------\n",
      "2017-11-22 23:08:12.699151: step 1564, loss = 0.7561 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.210938\n",
      "Validation top1 error = 0.2160\n",
      "Validation loss =  0.612973\n",
      "----------------------------\n",
      "2017-11-22 23:09:00.804597: step 1955, loss = 0.6326 (1170.4 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.1875\n",
      "Validation top1 error = 0.1840\n",
      "Validation loss =  0.66967\n",
      "----------------------------\n",
      "2017-11-22 23:09:49.052564: step 2346, loss = 0.5855 (1170.0 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.132813\n",
      "Validation top1 error = 0.1640\n",
      "Validation loss =  0.477928\n",
      "----------------------------\n",
      "2017-11-22 23:10:37.285423: step 2737, loss = 0.5473 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.117188\n",
      "Validation top1 error = 0.1320\n",
      "Validation loss =  0.456257\n",
      "----------------------------\n",
      "2017-11-22 23:11:25.377765: step 3128, loss = 0.6692 (925.9 examples/sec; 0.138 sec/batch)\n",
      "Train top1 error =  0.140625\n",
      "Validation top1 error = 0.1760\n",
      "Validation loss =  0.432742\n",
      "----------------------------\n",
      "2017-11-22 23:12:13.270377: step 3519, loss = 0.5243 (1170.5 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.132813\n",
      "Validation top1 error = 0.1400\n",
      "Validation loss =  0.458421\n",
      "----------------------------\n",
      "2017-11-22 23:13:01.111005: step 3910, loss = 0.4724 (1148.9 examples/sec; 0.111 sec/batch)\n",
      "Train top1 error =  0.117188\n",
      "Validation top1 error = 0.1880\n",
      "Validation loss =  0.600572\n",
      "----------------------------\n",
      "2017-11-22 23:13:48.923661: step 4301, loss = 0.6204 (1170.3 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.210938\n",
      "Validation top1 error = 0.1720\n",
      "Validation loss =  0.516487\n",
      "----------------------------\n",
      "2017-11-22 23:14:36.871548: step 4692, loss = 0.4296 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0859375\n",
      "Validation top1 error = 0.1960\n",
      "Validation loss =  0.622241\n",
      "----------------------------\n",
      "2017-11-22 23:15:24.882204: step 5083, loss = 0.5286 (1170.1 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.125\n",
      "Validation top1 error = 0.1280\n",
      "Validation loss =  0.446578\n",
      "----------------------------\n",
      "2017-11-22 23:16:12.784683: step 5474, loss = 0.6121 (1170.1 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.171875\n",
      "Validation top1 error = 0.1520\n",
      "Validation loss =  0.397181\n",
      "----------------------------\n",
      "2017-11-22 23:17:00.575098: step 5865, loss = 0.6413 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.164063\n",
      "Validation top1 error = 0.1360\n",
      "Validation loss =  0.453526\n",
      "----------------------------\n",
      "2017-11-22 23:17:48.735643: step 6256, loss = 0.7141 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.179688\n",
      "Validation top1 error = 0.1520\n",
      "Validation loss =  0.376978\n",
      "----------------------------\n",
      "2017-11-22 23:18:36.671150: step 6647, loss = 0.4822 (1238.3 examples/sec; 0.103 sec/batch)\n",
      "Train top1 error =  0.0703125\n",
      "Validation top1 error = 0.1720\n",
      "Validation loss =  0.500293\n",
      "----------------------------\n",
      "2017-11-22 23:19:24.625913: step 7038, loss = 0.5049 (1194.7 examples/sec; 0.107 sec/batch)\n",
      "Train top1 error =  0.101563\n",
      "Validation top1 error = 0.1200\n",
      "Validation loss =  0.350715\n",
      "----------------------------\n",
      "2017-11-22 23:20:12.723496: step 7429, loss = 0.5051 (1170.1 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.109375\n",
      "Validation top1 error = 0.1080\n",
      "Validation loss =  0.306942\n",
      "----------------------------\n",
      "2017-11-22 23:21:00.814502: step 7820, loss = 0.5195 (1142.5 examples/sec; 0.112 sec/batch)\n",
      "Train top1 error =  0.125\n",
      "Validation top1 error = 0.1480\n",
      "Validation loss =  0.474947\n",
      "----------------------------\n",
      "2017-11-22 23:21:49.018489: step 8211, loss = 0.5652 (1145.8 examples/sec; 0.112 sec/batch)\n",
      "Train top1 error =  0.140625\n",
      "Validation top1 error = 0.1000\n",
      "Validation loss =  0.349278\n",
      "----------------------------\n",
      "2017-11-22 23:22:37.101441: step 8602, loss = 0.5037 (1170.4 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.101563\n",
      "Validation top1 error = 0.0800\n",
      "Validation loss =  0.333524\n",
      "----------------------------\n",
      "2017-11-22 23:23:25.064691: step 8993, loss = 0.3761 (1170.3 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0625\n",
      "Validation top1 error = 0.1520\n",
      "Validation loss =  0.475441\n",
      "----------------------------\n",
      "2017-11-22 23:24:13.036661: step 9384, loss = 0.3993 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0703125\n",
      "Validation top1 error = 0.1720\n",
      "Validation loss =  0.52918\n",
      "----------------------------\n",
      "2017-11-22 23:25:01.193192: step 9775, loss = 0.4966 (1023.9 examples/sec; 0.125 sec/batch)\n",
      "Train top1 error =  0.0625\n",
      "Validation top1 error = 0.1360\n",
      "Validation loss =  0.392605\n",
      "----------------------------\n",
      "2017-11-22 23:25:53.128455: step 10166, loss = 0.4709 (1170.0 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0859375\n",
      "Validation top1 error = 0.1400\n",
      "Validation loss =  0.435407\n",
      "----------------------------\n",
      "2017-11-22 23:26:41.038706: step 10557, loss = 0.5696 (1170.4 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0859375\n",
      "Validation top1 error = 0.1320\n",
      "Validation loss =  0.401907\n",
      "----------------------------\n",
      "2017-11-22 23:27:29.276712: step 10948, loss = 0.3697 (1121.5 examples/sec; 0.114 sec/batch)\n",
      "Train top1 error =  0.0546875\n",
      "Validation top1 error = 0.1640\n",
      "Validation loss =  0.441795\n",
      "----------------------------\n",
      "2017-11-22 23:28:17.444962: step 11339, loss = 0.4027 (1170.1 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0546875\n",
      "Validation top1 error = 0.1160\n",
      "Validation loss =  0.346758\n",
      "----------------------------\n",
      "2017-11-22 23:29:05.409562: step 11730, loss = 0.5090 (1170.1 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.125\n",
      "Validation top1 error = 0.1440\n",
      "Validation loss =  0.387444\n",
      "----------------------------\n",
      "2017-11-22 23:29:53.408604: step 12121, loss = 0.4338 (1024.1 examples/sec; 0.125 sec/batch)\n",
      "Train top1 error =  0.0703125\n",
      "Validation top1 error = 0.1200\n",
      "Validation loss =  0.44016\n",
      "----------------------------\n",
      "2017-11-22 23:30:41.693557: step 12512, loss = 0.3749 (1024.0 examples/sec; 0.125 sec/batch)\n",
      "Train top1 error =  0.046875\n",
      "Validation top1 error = 0.1240\n",
      "Validation loss =  0.350743\n",
      "----------------------------\n",
      "2017-11-22 23:31:29.695756: step 12903, loss = 0.4495 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.078125\n",
      "Validation top1 error = 0.1000\n",
      "Validation loss =  0.296133\n",
      "----------------------------\n",
      "2017-11-22 23:32:17.720780: step 13294, loss = 0.3955 (1170.3 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.078125\n",
      "Validation top1 error = 0.1560\n",
      "Validation loss =  0.524079\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-22 23:33:05.592191: step 13685, loss = 0.6316 (1170.3 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.140625\n",
      "Validation top1 error = 0.1400\n",
      "Validation loss =  0.400623\n",
      "----------------------------\n",
      "2017-11-22 23:33:53.624316: step 14076, loss = 0.5594 (1024.2 examples/sec; 0.125 sec/batch)\n",
      "Train top1 error =  0.109375\n",
      "Validation top1 error = 0.1240\n",
      "Validation loss =  0.406946\n",
      "----------------------------\n",
      "2017-11-22 23:34:41.807147: step 14467, loss = 0.3305 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.015625\n",
      "Validation top1 error = 0.0840\n",
      "Validation loss =  0.293914\n",
      "----------------------------\n",
      "2017-11-22 23:35:29.770958: step 14858, loss = 0.5056 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0703125\n",
      "Validation top1 error = 0.1240\n",
      "Validation loss =  0.352718\n",
      "----------------------------\n",
      "2017-11-22 23:36:17.931258: step 15249, loss = 0.4413 (1170.3 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.078125\n",
      "Validation top1 error = 0.1440\n",
      "Validation loss =  0.433765\n",
      "----------------------------\n",
      "2017-11-22 23:37:05.943876: step 15640, loss = 0.3851 (1170.1 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0625\n",
      "Validation top1 error = 0.1680\n",
      "Validation loss =  0.468058\n",
      "----------------------------\n",
      "2017-11-22 23:37:53.971975: step 16031, loss = 0.4189 (1170.1 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0546875\n",
      "Validation top1 error = 0.1440\n",
      "Validation loss =  0.482096\n",
      "----------------------------\n",
      "2017-11-22 23:38:41.907169: step 16422, loss = 0.3436 (1170.1 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0546875\n",
      "Validation top1 error = 0.1320\n",
      "Validation loss =  0.328968\n",
      "----------------------------\n",
      "2017-11-22 23:39:30.134745: step 16813, loss = 0.5528 (1170.5 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.117188\n",
      "Validation top1 error = 0.1480\n",
      "Validation loss =  0.477455\n",
      "----------------------------\n",
      "2017-11-22 23:40:18.126482: step 17204, loss = 0.3378 (1170.4 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0390625\n",
      "Validation top1 error = 0.1240\n",
      "Validation loss =  0.436677\n",
      "----------------------------\n",
      "2017-11-22 23:41:06.345909: step 17595, loss = 0.4840 (1139.5 examples/sec; 0.112 sec/batch)\n",
      "Train top1 error =  0.0703125\n",
      "Validation top1 error = 0.1560\n",
      "Validation loss =  0.53454\n",
      "----------------------------\n",
      "2017-11-22 23:41:54.789019: step 17986, loss = 0.3411 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.03125\n",
      "Validation top1 error = 0.1320\n",
      "Validation loss =  0.420252\n",
      "----------------------------\n",
      "2017-11-22 23:42:42.867263: step 18377, loss = 0.6532 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.164063\n",
      "Validation top1 error = 0.1440\n",
      "Validation loss =  0.478564\n",
      "----------------------------\n",
      "2017-11-22 23:43:31.001370: step 18768, loss = 0.3650 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.046875\n",
      "Validation top1 error = 0.1280\n",
      "Validation loss =  0.424565\n",
      "----------------------------\n",
      "2017-11-22 23:44:19.202721: step 19159, loss = 0.4546 (1170.3 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.09375\n",
      "Validation top1 error = 0.1160\n",
      "Validation loss =  0.400005\n",
      "----------------------------\n",
      "2017-11-22 23:45:07.562061: step 19550, loss = 0.3790 (1170.5 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0390625\n",
      "Validation top1 error = 0.1160\n",
      "Validation loss =  0.384467\n",
      "----------------------------\n",
      "2017-11-22 23:45:55.669844: step 19941, loss = 0.4508 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0546875\n",
      "Validation top1 error = 0.1120\n",
      "Validation loss =  0.346078\n",
      "----------------------------\n",
      "2017-11-22 23:46:47.761198: step 20332, loss = 0.5184 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.109375\n",
      "Validation top1 error = 0.0960\n",
      "Validation loss =  0.274134\n",
      "----------------------------\n",
      "2017-11-22 23:47:35.858831: step 20723, loss = 0.5305 (1169.9 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.109375\n",
      "Validation top1 error = 0.1160\n",
      "Validation loss =  0.344142\n",
      "----------------------------\n",
      "2017-11-22 23:48:24.114465: step 21114, loss = 0.4012 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0625\n",
      "Validation top1 error = 0.1080\n",
      "Validation loss =  0.345148\n",
      "----------------------------\n",
      "2017-11-22 23:49:12.276040: step 21505, loss = 0.4893 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.078125\n",
      "Validation top1 error = 0.1120\n",
      "Validation loss =  0.360621\n",
      "----------------------------\n",
      "2017-11-22 23:50:00.447574: step 21896, loss = 0.4143 (1119.8 examples/sec; 0.114 sec/batch)\n",
      "Train top1 error =  0.0625\n",
      "Validation top1 error = 0.1440\n",
      "Validation loss =  0.367793\n",
      "----------------------------\n",
      "2017-11-22 23:50:48.583905: step 22287, loss = 0.3465 (1170.5 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0546875\n",
      "Validation top1 error = 0.1360\n",
      "Validation loss =  0.390692\n",
      "----------------------------\n",
      "2017-11-22 23:51:37.039411: step 22678, loss = 0.5034 (1170.0 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.078125\n",
      "Validation top1 error = 0.1560\n",
      "Validation loss =  0.483767\n",
      "----------------------------\n",
      "2017-11-22 23:52:25.614173: step 23069, loss = 0.4291 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.046875\n",
      "Validation top1 error = 0.1120\n",
      "Validation loss =  0.291874\n",
      "----------------------------\n",
      "2017-11-22 23:55:37.175927: step 23460, loss = 0.3997 (243.2 examples/sec; 0.526 sec/batch)\n",
      "Train top1 error =  0.046875\n",
      "Validation top1 error = 0.1600\n",
      "Validation loss =  0.512122\n",
      "----------------------------\n",
      "2017-11-22 23:56:38.074803: step 23851, loss = 0.3704 (1089.6 examples/sec; 0.117 sec/batch)\n",
      "Train top1 error =  0.0546875\n",
      "Validation top1 error = 0.1040\n",
      "Validation loss =  0.340969\n",
      "----------------------------\n",
      "2017-11-22 23:57:26.398203: step 24242, loss = 0.4754 (1038.4 examples/sec; 0.123 sec/batch)\n",
      "Train top1 error =  0.0625\n",
      "Validation top1 error = 0.1800\n",
      "Validation loss =  0.600379\n",
      "----------------------------\n",
      "2017-11-22 23:58:14.561543: step 24633, loss = 0.4750 (1170.4 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0859375\n",
      "Validation top1 error = 0.1400\n",
      "Validation loss =  0.301175\n",
      "----------------------------\n",
      "2017-11-22 23:59:02.856718: step 25024, loss = 0.3713 (1217.9 examples/sec; 0.105 sec/batch)\n",
      "Train top1 error =  0.0546875\n",
      "Validation top1 error = 0.1520\n",
      "Validation loss =  0.511542\n",
      "----------------------------\n",
      "2017-11-22 23:59:50.930232: step 25415, loss = 0.4557 (1170.3 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0859375\n",
      "Validation top1 error = 0.1360\n",
      "Validation loss =  0.370353\n",
      "----------------------------\n",
      "2017-11-23 00:00:39.315547: step 25806, loss = 0.3864 (1170.3 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0546875\n",
      "Validation top1 error = 0.1000\n",
      "Validation loss =  0.318742\n",
      "----------------------------\n",
      "2017-11-23 00:01:27.409920: step 26197, loss = 0.4393 (1170.3 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.078125\n",
      "Validation top1 error = 0.1080\n",
      "Validation loss =  0.326018\n",
      "----------------------------\n",
      "2017-11-23 00:02:15.333324: step 26588, loss = 0.4609 (1170.3 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.078125\n",
      "Validation top1 error = 0.0760\n",
      "Validation loss =  0.278053\n",
      "----------------------------\n",
      "2017-11-23 00:03:03.398305: step 26979, loss = 0.4254 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.046875\n",
      "Validation top1 error = 0.1160\n",
      "Validation loss =  0.307429\n",
      "----------------------------\n",
      "2017-11-23 00:03:51.403331: step 27370, loss = 0.5130 (1169.9 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.101563\n",
      "Validation top1 error = 0.1120\n",
      "Validation loss =  0.411354\n",
      "----------------------------\n",
      "2017-11-23 00:04:39.338603: step 27761, loss = 0.4475 (1170.5 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0546875\n",
      "Validation top1 error = 0.1280\n",
      "Validation loss =  0.378359\n",
      "----------------------------\n",
      "2017-11-23 00:05:27.261468: step 28152, loss = 0.3552 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.046875\n",
      "Validation top1 error = 0.1080\n",
      "Validation loss =  0.385387\n",
      "----------------------------\n",
      "2017-11-23 00:06:15.340935: step 28543, loss = 0.3811 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.03125\n",
      "Validation top1 error = 0.0920\n",
      "Validation loss =  0.255001\n",
      "----------------------------\n",
      "2017-11-23 00:07:03.375178: step 28934, loss = 0.4633 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0703125\n",
      "Validation top1 error = 0.1360\n",
      "Validation loss =  0.39678\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-23 00:07:51.360399: step 29325, loss = 0.4553 (1169.9 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.078125\n",
      "Validation top1 error = 0.1080\n",
      "Validation loss =  0.28443\n",
      "----------------------------\n",
      "2017-11-23 00:08:39.423697: step 29716, loss = 0.3179 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.03125\n",
      "Validation top1 error = 0.1120\n",
      "Validation loss =  0.356484\n",
      "----------------------------\n",
      "2017-11-23 00:09:31.181153: step 30107, loss = 0.4319 (1170.5 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0546875\n",
      "Validation top1 error = 0.1480\n",
      "Validation loss =  0.42558\n",
      "----------------------------\n",
      "2017-11-23 00:10:19.176460: step 30498, loss = 0.4658 (1023.8 examples/sec; 0.125 sec/batch)\n",
      "Train top1 error =  0.09375\n",
      "Validation top1 error = 0.0880\n",
      "Validation loss =  0.29418\n",
      "----------------------------\n",
      "2017-11-23 00:11:07.549873: step 30889, loss = 0.4102 (1024.1 examples/sec; 0.125 sec/batch)\n",
      "Train top1 error =  0.0546875\n",
      "Validation top1 error = 0.1040\n",
      "Validation loss =  0.400527\n",
      "----------------------------\n",
      "2017-11-23 00:11:56.034878: step 31280, loss = 0.4033 (1129.9 examples/sec; 0.113 sec/batch)\n",
      "Train top1 error =  0.0546875\n",
      "Validation top1 error = 0.1160\n",
      "Validation loss =  0.305232\n",
      "----------------------------\n",
      "2017-11-23 00:12:44.073260: step 31671, loss = 0.6103 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.117188\n",
      "Validation top1 error = 0.1040\n",
      "Validation loss =  0.367155\n",
      "----------------------------\n",
      "2017-11-23 00:13:32.213715: step 32062, loss = 0.4069 (994.2 examples/sec; 0.129 sec/batch)\n",
      "Train top1 error =  0.0546875\n",
      "Validation top1 error = 0.1040\n",
      "Validation loss =  0.338024\n",
      "----------------------------\n",
      "2017-11-23 00:14:20.168386: step 32453, loss = 0.5290 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.101563\n",
      "Validation top1 error = 0.1480\n",
      "Validation loss =  0.441165\n",
      "----------------------------\n",
      "2017-11-23 00:15:08.329897: step 32844, loss = 0.4070 (1307.8 examples/sec; 0.098 sec/batch)\n",
      "Train top1 error =  0.0546875\n",
      "Validation top1 error = 0.1120\n",
      "Validation loss =  0.361024\n",
      "----------------------------\n",
      "2017-11-23 00:15:56.433937: step 33235, loss = 0.4283 (1170.5 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0625\n",
      "Validation top1 error = 0.1280\n",
      "Validation loss =  0.397903\n",
      "----------------------------\n",
      "2017-11-23 00:16:44.449208: step 33626, loss = 0.2940 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0234375\n",
      "Validation top1 error = 0.1400\n",
      "Validation loss =  0.430668\n",
      "----------------------------\n",
      "2017-11-23 00:17:32.385285: step 34017, loss = 0.4353 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0546875\n",
      "Validation top1 error = 0.1040\n",
      "Validation loss =  0.311951\n",
      "----------------------------\n",
      "2017-11-23 00:18:20.374322: step 34408, loss = 0.3080 (1170.1 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0234375\n",
      "Validation top1 error = 0.1360\n",
      "Validation loss =  0.396962\n",
      "----------------------------\n",
      "2017-11-23 00:19:08.383299: step 34799, loss = 0.4188 (1170.4 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0703125\n",
      "Validation top1 error = 0.1400\n",
      "Validation loss =  0.482927\n",
      "----------------------------\n",
      "2017-11-23 00:19:56.305198: step 35190, loss = 0.5977 (1169.9 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.09375\n",
      "Validation top1 error = 0.1560\n",
      "Validation loss =  0.478313\n",
      "----------------------------\n",
      "2017-11-23 00:20:44.424672: step 35581, loss = 0.3600 (1169.9 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.046875\n",
      "Validation top1 error = 0.1080\n",
      "Validation loss =  0.373287\n",
      "----------------------------\n",
      "2017-11-23 00:21:32.422555: step 35972, loss = 0.4090 (1170.0 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0625\n",
      "Validation top1 error = 0.1240\n",
      "Validation loss =  0.341569\n",
      "----------------------------\n",
      "2017-11-23 00:22:20.461791: step 36363, loss = 0.3508 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.03125\n",
      "Validation top1 error = 0.1040\n",
      "Validation loss =  0.309172\n",
      "----------------------------\n",
      "2017-11-23 00:23:08.470949: step 36754, loss = 0.4025 (1169.8 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0625\n",
      "Validation top1 error = 0.1240\n",
      "Validation loss =  0.416288\n",
      "----------------------------\n",
      "2017-11-23 00:23:56.416857: step 37145, loss = 0.4135 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0625\n",
      "Validation top1 error = 0.1240\n",
      "Validation loss =  0.363687\n",
      "----------------------------\n",
      "2017-11-23 00:24:44.424668: step 37536, loss = 0.3735 (1170.7 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0546875\n",
      "Validation top1 error = 0.1200\n",
      "Validation loss =  0.369866\n",
      "----------------------------\n",
      "2017-11-23 00:25:32.404652: step 37927, loss = 0.4477 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0625\n",
      "Validation top1 error = 0.1440\n",
      "Validation loss =  0.371089\n",
      "----------------------------\n",
      "2017-11-23 00:26:20.262249: step 38318, loss = 0.4523 (1169.9 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0625\n",
      "Validation top1 error = 0.1200\n",
      "Validation loss =  0.375406\n",
      "----------------------------\n",
      "2017-11-23 00:27:08.191998: step 38709, loss = 0.3800 (1170.3 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0625\n",
      "Validation top1 error = 0.1480\n",
      "Validation loss =  0.469059\n",
      "----------------------------\n",
      "2017-11-23 00:27:56.166560: step 39100, loss = 0.3816 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0625\n",
      "Validation top1 error = 0.1120\n",
      "Validation loss =  0.33933\n",
      "----------------------------\n",
      "2017-11-23 00:28:44.059394: step 39491, loss = 0.4880 (1170.4 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.078125\n",
      "Validation top1 error = 0.1120\n",
      "Validation loss =  0.411533\n",
      "----------------------------\n",
      "2017-11-23 00:29:31.825865: step 39882, loss = 0.4441 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.078125\n",
      "Validation top1 error = 0.1360\n",
      "Validation loss =  0.421183\n",
      "----------------------------\n",
      "Learning rate decayed to  0.010000000000000002\n",
      "2017-11-23 00:30:23.631827: step 40273, loss = 0.3158 (1170.5 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0078125\n",
      "Validation top1 error = 0.0920\n",
      "Validation loss =  0.339145\n",
      "----------------------------\n",
      "2017-11-23 00:31:11.442881: step 40664, loss = 0.2947 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0234375\n",
      "Validation top1 error = 0.1000\n",
      "Validation loss =  0.302699\n",
      "----------------------------\n",
      "2017-11-23 00:31:59.320010: step 41055, loss = 0.2723 (1169.9 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0078125\n",
      "Validation top1 error = 0.0760\n",
      "Validation loss =  0.301202\n",
      "----------------------------\n",
      "2017-11-23 00:32:47.235531: step 41446, loss = 0.2759 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.015625\n",
      "Validation top1 error = 0.0720\n",
      "Validation loss =  0.262283\n",
      "----------------------------\n",
      "2017-11-23 00:33:35.137051: step 41837, loss = 0.2868 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.015625\n",
      "Validation top1 error = 0.0680\n",
      "Validation loss =  0.257684\n",
      "----------------------------\n",
      "2017-11-23 00:34:23.006078: step 42228, loss = 0.2637 (1024.1 examples/sec; 0.125 sec/batch)\n",
      "Train top1 error =  0.0078125\n",
      "Validation top1 error = 0.0720\n",
      "Validation loss =  0.274646\n",
      "----------------------------\n",
      "2017-11-23 00:35:10.911550: step 42619, loss = 0.2815 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.03125\n",
      "Validation top1 error = 0.1000\n",
      "Validation loss =  0.322133\n",
      "----------------------------\n",
      "2017-11-23 06:59:35.914326: step 43010, loss = 0.2292 (1170.5 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.1000\n",
      "Validation loss =  0.380909\n",
      "----------------------------\n",
      "2017-11-23 07:00:23.973914: step 43401, loss = 0.2311 (1024.1 examples/sec; 0.125 sec/batch)\n",
      "Train top1 error =  0.0078125\n",
      "Validation top1 error = 0.0760\n",
      "Validation loss =  0.298024\n",
      "----------------------------\n",
      "2017-11-23 07:01:12.010566: step 43792, loss = 0.2266 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0920\n",
      "Validation loss =  0.319221\n",
      "----------------------------\n",
      "2017-11-23 07:02:00.418402: step 44183, loss = 0.2349 (1110.4 examples/sec; 0.115 sec/batch)\n",
      "Train top1 error =  0.0078125\n",
      "Validation top1 error = 0.0840\n",
      "Validation loss =  0.337391\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-23 07:02:49.220283: step 44574, loss = 0.2073 (1129.9 examples/sec; 0.113 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0920\n",
      "Validation loss =  0.318431\n",
      "----------------------------\n",
      "2017-11-23 07:03:37.555912: step 44965, loss = 0.2068 (1160.5 examples/sec; 0.110 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0800\n",
      "Validation loss =  0.259846\n",
      "----------------------------\n",
      "2017-11-23 07:04:25.951486: step 45356, loss = 0.2037 (1149.7 examples/sec; 0.111 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0640\n",
      "Validation loss =  0.245605\n",
      "----------------------------\n",
      "2017-11-23 07:05:14.523199: step 45747, loss = 0.2151 (1155.3 examples/sec; 0.111 sec/batch)\n",
      "Train top1 error =  0.0078125\n",
      "Validation top1 error = 0.1000\n",
      "Validation loss =  0.368235\n",
      "----------------------------\n",
      "2017-11-23 07:06:03.112001: step 46138, loss = 0.2102 (1155.3 examples/sec; 0.111 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0680\n",
      "Validation loss =  0.279405\n",
      "----------------------------\n",
      "2017-11-23 07:06:51.515524: step 46529, loss = 0.2252 (1160.9 examples/sec; 0.110 sec/batch)\n",
      "Train top1 error =  0.015625\n",
      "Validation top1 error = 0.0600\n",
      "Validation loss =  0.255608\n",
      "----------------------------\n",
      "2017-11-23 07:07:39.773808: step 46920, loss = 0.2157 (1165.8 examples/sec; 0.110 sec/batch)\n",
      "Train top1 error =  0.015625\n",
      "Validation top1 error = 0.1240\n",
      "Validation loss =  0.388181\n",
      "----------------------------\n",
      "2017-11-23 07:08:28.087682: step 47311, loss = 0.1922 (1029.3 examples/sec; 0.124 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0800\n",
      "Validation loss =  0.277493\n",
      "----------------------------\n",
      "2017-11-23 07:09:16.817635: step 47702, loss = 0.1905 (1145.0 examples/sec; 0.112 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0760\n",
      "Validation loss =  0.215085\n",
      "----------------------------\n",
      "2017-11-23 07:10:05.391849: step 48093, loss = 0.1895 (1149.9 examples/sec; 0.111 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0800\n",
      "Validation loss =  0.289845\n",
      "----------------------------\n",
      "2017-11-23 07:10:53.868635: step 48484, loss = 0.1787 (1160.5 examples/sec; 0.110 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.1000\n",
      "Validation loss =  0.316283\n",
      "----------------------------\n",
      "2017-11-23 07:11:42.143201: step 48875, loss = 0.1765 (1105.0 examples/sec; 0.116 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.1080\n",
      "Validation loss =  0.586522\n",
      "----------------------------\n",
      "2017-11-23 07:12:30.556889: step 49266, loss = 0.1839 (1150.1 examples/sec; 0.111 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.1040\n",
      "Validation loss =  0.452227\n",
      "----------------------------\n",
      "2017-11-23 07:13:18.827211: step 49657, loss = 0.1757 (1150.1 examples/sec; 0.111 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0560\n",
      "Validation loss =  0.296563\n",
      "----------------------------\n",
      "2017-11-23 07:14:11.592560: step 50048, loss = 0.1742 (1160.2 examples/sec; 0.110 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0800\n",
      "Validation loss =  0.335882\n",
      "----------------------------\n",
      "2017-11-23 07:14:59.956533: step 50439, loss = 0.1703 (1155.1 examples/sec; 0.111 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0840\n",
      "Validation loss =  0.392373\n",
      "----------------------------\n",
      "2017-11-23 07:15:48.192708: step 50830, loss = 0.1732 (1124.8 examples/sec; 0.114 sec/batch)\n",
      "Train top1 error =  0.0078125\n",
      "Validation top1 error = 0.0840\n",
      "Validation loss =  0.351581\n",
      "----------------------------\n",
      "2017-11-23 07:16:36.552644: step 51221, loss = 0.1658 (1150.1 examples/sec; 0.111 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.1080\n",
      "Validation loss =  0.428185\n",
      "----------------------------\n",
      "2017-11-23 07:17:24.741242: step 51612, loss = 0.1686 (1276.2 examples/sec; 0.100 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0960\n",
      "Validation loss =  0.534586\n",
      "----------------------------\n",
      "2017-11-23 07:18:12.707607: step 52003, loss = 0.1570 (1104.4 examples/sec; 0.116 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.1240\n",
      "Validation loss =  0.556022\n",
      "----------------------------\n",
      "2017-11-23 07:19:00.854257: step 52394, loss = 0.1565 (1104.0 examples/sec; 0.116 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0640\n",
      "Validation loss =  0.254885\n",
      "----------------------------\n",
      "2017-11-23 07:19:49.077848: step 52785, loss = 0.1514 (1049.4 examples/sec; 0.122 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0840\n",
      "Validation loss =  0.451368\n",
      "----------------------------\n",
      "2017-11-23 07:20:37.187039: step 53176, loss = 0.1511 (1276.8 examples/sec; 0.100 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.1080\n",
      "Validation loss =  0.521282\n",
      "----------------------------\n",
      "2017-11-23 07:21:25.229777: step 53567, loss = 0.1575 (1049.7 examples/sec; 0.122 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0680\n",
      "Validation loss =  0.31926\n",
      "----------------------------\n",
      "2017-11-23 07:22:13.475028: step 53958, loss = 0.1481 (1106.8 examples/sec; 0.116 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0960\n",
      "Validation loss =  0.459597\n",
      "----------------------------\n",
      "2017-11-23 07:23:01.478176: step 54349, loss = 0.1450 (1262.8 examples/sec; 0.101 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0640\n",
      "Validation loss =  0.325587\n",
      "----------------------------\n",
      "2017-11-23 07:23:49.437410: step 54740, loss = 0.1421 (1080.7 examples/sec; 0.118 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0640\n",
      "Validation loss =  0.324034\n",
      "----------------------------\n",
      "2017-11-23 07:24:37.598549: step 55131, loss = 0.1527 (1108.0 examples/sec; 0.116 sec/batch)\n",
      "Train top1 error =  0.0078125\n",
      "Validation top1 error = 0.1040\n",
      "Validation loss =  0.408974\n",
      "----------------------------\n",
      "2017-11-23 07:25:25.614637: step 55522, loss = 0.1398 (1101.6 examples/sec; 0.116 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.1000\n",
      "Validation loss =  0.478221\n",
      "----------------------------\n",
      "2017-11-23 07:26:13.804161: step 55913, loss = 0.1479 (1104.7 examples/sec; 0.116 sec/batch)\n",
      "Train top1 error =  0.0078125\n",
      "Validation top1 error = 0.0880\n",
      "Validation loss =  0.46686\n",
      "----------------------------\n",
      "2017-11-23 07:27:01.788697: step 56304, loss = 0.1356 (1104.3 examples/sec; 0.116 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0840\n",
      "Validation loss =  0.543315\n",
      "----------------------------\n",
      "2017-11-23 07:27:49.793668: step 56695, loss = 0.1436 (1281.2 examples/sec; 0.100 sec/batch)\n",
      "Train top1 error =  0.0078125\n",
      "Validation top1 error = 0.0600\n",
      "Validation loss =  0.315563\n",
      "----------------------------\n",
      "2017-11-23 07:28:37.854182: step 57086, loss = 0.1353 (1103.9 examples/sec; 0.116 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0800\n",
      "Validation loss =  0.294459\n",
      "----------------------------\n",
      "2017-11-23 07:29:26.020839: step 57477, loss = 0.1284 (1100.9 examples/sec; 0.116 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.1000\n",
      "Validation loss =  0.457185\n",
      "----------------------------\n",
      "2017-11-23 07:30:14.004121: step 57868, loss = 0.1298 (1104.3 examples/sec; 0.116 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0600\n",
      "Validation loss =  0.394324\n",
      "----------------------------\n",
      "2017-11-23 07:31:01.999537: step 58259, loss = 0.1261 (1107.5 examples/sec; 0.116 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0880\n",
      "Validation loss =  0.353538\n",
      "----------------------------\n",
      "2017-11-23 07:31:50.184902: step 58650, loss = 0.1261 (1104.4 examples/sec; 0.116 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0760\n",
      "Validation loss =  0.283725\n",
      "----------------------------\n",
      "2017-11-23 07:32:38.344960: step 59041, loss = 0.1257 (1109.3 examples/sec; 0.115 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0800\n",
      "Validation loss =  0.364001\n",
      "----------------------------\n",
      "2017-11-23 07:33:26.415268: step 59432, loss = 0.1222 (1152.4 examples/sec; 0.111 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0760\n",
      "Validation loss =  0.299358\n",
      "----------------------------\n",
      "2017-11-23 07:34:14.743489: step 59823, loss = 0.1247 (1054.0 examples/sec; 0.121 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0720\n",
      "Validation loss =  0.34884\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate decayed to  0.0010000000000000002\n",
      "2017-11-23 07:35:06.917640: step 60214, loss = 0.1186 (1104.3 examples/sec; 0.116 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0560\n",
      "Validation loss =  0.2837\n",
      "----------------------------\n",
      "2017-11-23 07:35:55.274902: step 60605, loss = 0.1173 (1169.9 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.1120\n",
      "Validation loss =  0.550896\n",
      "----------------------------\n",
      "2017-11-23 07:36:43.316649: step 60996, loss = 0.1179 (1170.0 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0960\n",
      "Validation loss =  0.452291\n",
      "----------------------------\n",
      "2017-11-23 07:37:31.269175: step 61387, loss = 0.1213 (1170.3 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0640\n",
      "Validation loss =  0.284499\n",
      "----------------------------\n",
      "2017-11-23 07:38:19.329508: step 61778, loss = 0.1316 (1169.9 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0078125\n",
      "Validation top1 error = 0.0600\n",
      "Validation loss =  0.378552\n",
      "----------------------------\n",
      "2017-11-23 07:39:07.221352: step 62169, loss = 0.1172 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0800\n",
      "Validation loss =  0.393749\n",
      "----------------------------\n",
      "2017-11-23 07:39:55.155192: step 62560, loss = 0.1174 (1169.9 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0720\n",
      "Validation loss =  0.464803\n",
      "----------------------------\n",
      "2017-11-23 07:40:43.125607: step 62951, loss = 0.1178 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.1000\n",
      "Validation loss =  0.503562\n",
      "----------------------------\n",
      "2017-11-23 07:41:31.229389: step 63342, loss = 0.1528 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0078125\n",
      "Validation top1 error = 0.0760\n",
      "Validation loss =  0.284297\n",
      "----------------------------\n",
      "2017-11-23 07:42:19.061827: step 63733, loss = 0.1177 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.1000\n",
      "Validation loss =  0.531886\n",
      "----------------------------\n",
      "2017-11-23 07:43:07.025998: step 64124, loss = 0.1164 (1170.4 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0760\n",
      "Validation loss =  0.379752\n",
      "----------------------------\n",
      "2017-11-23 07:43:55.054204: step 64515, loss = 0.1178 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0760\n",
      "Validation loss =  0.471486\n",
      "----------------------------\n",
      "2017-11-23 07:44:43.124252: step 64906, loss = 0.1166 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0600\n",
      "Validation loss =  0.28327\n",
      "----------------------------\n",
      "2017-11-23 07:45:31.082368: step 65297, loss = 0.1154 (1169.9 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0840\n",
      "Validation loss =  0.489509\n",
      "----------------------------\n",
      "2017-11-23 07:46:19.215865: step 65688, loss = 0.1168 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0640\n",
      "Validation loss =  0.359508\n",
      "----------------------------\n",
      "2017-11-23 07:47:07.115562: step 66079, loss = 0.1153 (1170.1 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0680\n",
      "Validation loss =  0.391493\n",
      "----------------------------\n",
      "2017-11-23 07:47:55.124487: step 66470, loss = 0.1165 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0800\n",
      "Validation loss =  0.473205\n",
      "----------------------------\n",
      "2017-11-23 07:48:43.028281: step 66861, loss = 0.1142 (1169.9 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0640\n",
      "Validation loss =  0.34155\n",
      "----------------------------\n",
      "2017-11-23 07:49:30.872905: step 67252, loss = 0.1151 (1170.4 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0680\n",
      "Validation loss =  0.325362\n",
      "----------------------------\n",
      "2017-11-23 07:50:18.746361: step 67643, loss = 0.1153 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0600\n",
      "Validation loss =  0.33339\n",
      "----------------------------\n",
      "2017-11-23 07:51:06.642005: step 68034, loss = 0.1168 (1169.9 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0800\n",
      "Validation loss =  0.363497\n",
      "----------------------------\n",
      "2017-11-23 07:51:54.474933: step 68425, loss = 0.1131 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.1240\n",
      "Validation loss =  0.506133\n",
      "----------------------------\n",
      "2017-11-23 07:52:42.289184: step 68816, loss = 0.1214 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0078125\n",
      "Validation top1 error = 0.1000\n",
      "Validation loss =  0.419133\n",
      "----------------------------\n",
      "2017-11-23 07:53:30.180976: step 69207, loss = 0.1144 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0920\n",
      "Validation loss =  0.331394\n",
      "----------------------------\n",
      "2017-11-23 07:54:18.035738: step 69598, loss = 0.1128 (1169.9 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0520\n",
      "Validation loss =  0.176617\n",
      "----------------------------\n",
      "2017-11-23 07:55:05.861077: step 69989, loss = 0.1165 (1169.9 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0720\n",
      "Validation loss =  0.390635\n",
      "----------------------------\n",
      "2017-11-23 07:55:57.770723: step 70380, loss = 0.1145 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0800\n",
      "Validation loss =  0.241842\n",
      "----------------------------\n",
      "2017-11-23 15:25:08.505532: step 70771, loss = 0.1131 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0760\n",
      "Validation loss =  0.336534\n",
      "----------------------------\n",
      "2017-11-23 15:25:56.386960: step 71162, loss = 0.1194 (1170.4 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0720\n",
      "Validation loss =  0.309619\n",
      "----------------------------\n",
      "2017-11-23 15:26:44.313538: step 71553, loss = 0.1126 (1169.9 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0880\n",
      "Validation loss =  0.403416\n",
      "----------------------------\n",
      "2017-11-23 15:27:32.064884: step 71944, loss = 0.1121 (1170.1 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.1160\n",
      "Validation loss =  0.463497\n",
      "----------------------------\n",
      "2017-11-23 15:28:20.167269: step 72335, loss = 0.1111 (1169.9 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0840\n",
      "Validation loss =  0.486982\n",
      "----------------------------\n",
      "2017-11-23 15:29:07.991377: step 72726, loss = 0.1138 (1170.5 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0560\n",
      "Validation loss =  0.298245\n",
      "----------------------------\n",
      "2017-11-23 15:29:55.875183: step 73117, loss = 0.1136 (1169.9 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0800\n",
      "Validation loss =  0.32056\n",
      "----------------------------\n",
      "2017-11-23 15:30:43.912669: step 73508, loss = 0.1179 (1170.1 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0520\n",
      "Validation loss =  0.289296\n",
      "----------------------------\n",
      "2017-11-23 15:31:31.789913: step 73899, loss = 0.1122 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.1000\n",
      "Validation loss =  0.531375\n",
      "----------------------------\n",
      "2017-11-23 15:32:19.594594: step 74290, loss = 0.1129 (1170.1 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.1000\n",
      "Validation loss =  0.512653\n",
      "----------------------------\n",
      "2017-11-23 15:33:07.542936: step 74681, loss = 0.1121 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0600\n",
      "Validation loss =  0.278535\n",
      "----------------------------\n",
      "2017-11-23 15:33:55.396225: step 75072, loss = 0.1102 (1023.9 examples/sec; 0.125 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0720\n",
      "Validation loss =  0.285248\n",
      "----------------------------\n",
      "2017-11-23 15:34:43.401384: step 75463, loss = 0.1145 (1170.3 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0760\n",
      "Validation loss =  0.396805\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-23 15:35:31.412532: step 75854, loss = 0.1128 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.1000\n",
      "Validation loss =  0.487078\n",
      "----------------------------\n",
      "2017-11-23 15:36:19.306708: step 76245, loss = 0.1100 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0760\n",
      "Validation loss =  0.411567\n",
      "----------------------------\n",
      "2017-11-23 15:37:07.142607: step 76636, loss = 0.1120 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0560\n",
      "Validation loss =  0.279281\n",
      "----------------------------\n",
      "2017-11-23 15:37:54.976532: step 77027, loss = 0.1101 (1024.2 examples/sec; 0.125 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0960\n",
      "Validation loss =  0.406749\n",
      "----------------------------\n",
      "2017-11-23 15:38:42.900768: step 77418, loss = 0.1101 (1170.1 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0840\n",
      "Validation loss =  0.451882\n",
      "----------------------------\n",
      "2017-11-23 15:39:30.729063: step 77809, loss = 0.1091 (1170.4 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0720\n",
      "Validation loss =  0.276587\n",
      "----------------------------\n",
      "2017-11-23 15:40:18.871910: step 78200, loss = 0.1102 (1170.2 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.1000\n",
      "Validation loss =  0.529538\n",
      "----------------------------\n",
      "2017-11-23 15:41:06.848515: step 78591, loss = 0.1091 (1169.9 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0800\n",
      "Validation loss =  0.301953\n",
      "----------------------------\n",
      "2017-11-23 15:41:54.774481: step 78982, loss = 0.1111 (1169.9 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0640\n",
      "Validation loss =  0.3084\n",
      "----------------------------\n",
      "2017-11-23 15:42:42.713688: step 79373, loss = 0.1129 (1169.9 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.0800\n",
      "Validation loss =  0.345487\n",
      "----------------------------\n",
      "2017-11-23 15:43:30.626883: step 79764, loss = 0.1100 (1169.9 examples/sec; 0.109 sec/batch)\n",
      "Train top1 error =  0.0\n",
      "Validation top1 error = 0.1040\n",
      "Validation loss =  0.552675\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "from cifar10_input import *\n",
    "\n",
    "maybe_download_and_extract()\n",
    "# Initialize the Train object\n",
    "train = Train()\n",
    "# Start the training session\n",
    "train.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable conv0/conv already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\WENWEN\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"C:\\Users\\WENWEN\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\WENWEN\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-7f17eb92cb48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\resnet\\resnet.py\u001b[0m in \u001b[0;36mtest_graph\u001b[1;34m(train_dir)\u001b[0m\n\u001b[0;32m    214\u001b[0m     '''\n\u001b[0;32m    215\u001b[0m     \u001b[0minput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m     \u001b[0minit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m     \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\resnet\\resnet.py\u001b[0m in \u001b[0;36minference\u001b[1;34m(input_tensor_batch, n, reuse)\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[0mlayers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'conv0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[0mconv0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_bn_relu_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m         \u001b[0mactivation_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\resnet\\resnet.py\u001b[0m in \u001b[0;36mconv_bn_relu_layer\u001b[1;34m(input_layer, filter_shape, stride)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0mout_channel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilter_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[0mfilter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'conv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilter_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[0mconv_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'SAME'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\resnet\\resnet.py\u001b[0m in \u001b[0;36mcreate_variables\u001b[1;34m(name, shape, initializer, is_fc_layer)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     new_variables = tf.get_variable(name, shape=shape, initializer=initializer,\n\u001b[1;32m---> 40\u001b[1;33m                                     regularizer=regularizer)\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1201\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1203\u001b[1;33m       constraint=constraint)\n\u001b[0m\u001b[0;32m   1204\u001b[0m get_variable_or_local_docstring = (\n\u001b[0;32m   1205\u001b[0m     \"\"\"%s\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1090\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1092\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m   1093\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1094\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m    423\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    740\u001b[0m                          \u001b[1;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[1;32m--> 742\u001b[1;33m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    743\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable conv0/conv already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\WENWEN\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"C:\\Users\\WENWEN\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\WENWEN\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n"
     ]
    }
   ],
   "source": [
    "test_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
