{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from hyper_parameters import *\n",
    "\n",
    "BN_EPSILON = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activation_summary(x):\n",
    "    '''\n",
    "    :param x: A Tensor\n",
    "    :return: Add histogram summary and scalar summary of the sparsity of the tensor\n",
    "    '''\n",
    "    tensor_name = x.op.name\n",
    "    tf.summary.histogram(tensor_name + '/activations', x)\n",
    "    tf.summary.scalar(tensor_name + '/sparsity', tf.nn.zero_fraction(x))\n",
    "\n",
    "\n",
    "def create_variables(name, shape, initializer=tf.contrib.layers.xavier_initializer(), is_fc_layer=False):\n",
    "    '''\n",
    "    :param name: A string. The name of the new variable\n",
    "    :param shape: A list of dimensions\n",
    "    :param initializer: User Xavier as default.\n",
    "    :param is_fc_layer: Want to create fc layer variable? May use different weight_decay for fc\n",
    "    layers.\n",
    "    :return: The created variable\n",
    "    '''\n",
    "    \n",
    "    ## TODO: to allow different weight decay to fully connected layer and conv layer\n",
    "    if is_fc_layer is True:\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale=FLAGS.weight_decay)\n",
    "    else:\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale=FLAGS.weight_decay)\n",
    "\n",
    "    new_variables = tf.get_variable(name, shape=shape, initializer=initializer,\n",
    "                                    regularizer=regularizer)\n",
    "    return new_variables\n",
    "\n",
    "\n",
    "def output_layer(input_layer, num_labels):\n",
    "    '''\n",
    "    :param input_layer: 2D tensor\n",
    "    :param num_labels: int. How many output labels in total? (10 for cifar10 and 100 for cifar100)\n",
    "    :return: output layer Y = WX + B\n",
    "    '''\n",
    "    input_dim = input_layer.get_shape().as_list()[-1]\n",
    "    fc_w = create_variables(name='fc_weights', shape=[input_dim, num_labels], is_fc_layer=True,\n",
    "                            initializer=tf.uniform_unit_scaling_initializer(factor=1.0))\n",
    "    fc_b = create_variables(name='fc_bias', shape=[num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "    fc_h = tf.matmul(input_layer, fc_w) + fc_b\n",
    "    return fc_h\n",
    "\n",
    "\n",
    "def batch_normalization_layer(input_layer, dimension):\n",
    "    '''\n",
    "    Helper function to do batch normalziation\n",
    "    :param input_layer: 4D tensor\n",
    "    :param dimension: input_layer.get_shape().as_list()[-1]. The depth of the 4D tensor\n",
    "    :return: the 4D tensor after being normalized\n",
    "    '''\n",
    "    mean, variance = tf.nn.moments(input_layer, axes=[0, 1, 2])\n",
    "    beta = tf.get_variable('beta', dimension, tf.float32,\n",
    "                               initializer=tf.constant_initializer(0.0, tf.float32))\n",
    "    gamma = tf.get_variable('gamma', dimension, tf.float32,\n",
    "                                initializer=tf.constant_initializer(1.0, tf.float32))\n",
    "    bn_layer = tf.nn.batch_normalization(input_layer, mean, variance, beta, gamma, BN_EPSILON)\n",
    "\n",
    "    return bn_layer\n",
    "\n",
    "\n",
    "def conv_bn_relu_layer(input_layer, filter_shape, stride):\n",
    "    '''\n",
    "    A helper function to conv, batch normalize and relu the input tensor sequentially\n",
    "    :param input_layer: 4D tensor\n",
    "    :param filter_shape: list. [filter_height, filter_width, filter_depth, filter_number]\n",
    "    :param stride: stride size for conv\n",
    "    :return: 4D tensor. Y = Relu(batch_normalize(conv(X)))\n",
    "    '''\n",
    "\n",
    "    out_channel = filter_shape[-1]\n",
    "    filter = create_variables(name='conv', shape=filter_shape)\n",
    "\n",
    "    conv_layer = tf.nn.conv2d(input_layer, filter, strides=[1, stride, stride, 1], padding='SAME')\n",
    "    bn_layer = batch_normalization_layer(conv_layer, out_channel)\n",
    "\n",
    "    output = tf.nn.relu(bn_layer)\n",
    "    return output\n",
    "\n",
    "\n",
    "def bn_relu_conv_layer(input_layer, filter_shape, stride):\n",
    "    '''\n",
    "    A helper function to batch normalize, relu and conv the input layer sequentially\n",
    "    :param input_layer: 4D tensor\n",
    "    :param filter_shape: list. [filter_height, filter_width, filter_depth, filter_number]\n",
    "    :param stride: stride size for conv\n",
    "    :return: 4D tensor. Y = conv(Relu(batch_normalize(X)))\n",
    "    '''\n",
    "\n",
    "    in_channel = input_layer.get_shape().as_list()[-1]\n",
    "\n",
    "    bn_layer = batch_normalization_layer(input_layer, in_channel)\n",
    "    relu_layer = tf.nn.relu(bn_layer)\n",
    "\n",
    "    filter = create_variables(name='conv', shape=filter_shape)\n",
    "    conv_layer = tf.nn.conv2d(relu_layer, filter, strides=[1, stride, stride, 1], padding='SAME')\n",
    "    return conv_layer\n",
    "\n",
    "\n",
    "\n",
    "def residual_block(input_layer, output_channel, first_block=False):\n",
    "    '''\n",
    "    Defines a residual block in ResNet\n",
    "    :param input_layer: 4D tensor\n",
    "    :param output_channel: int. return_tensor.get_shape().as_list()[-1] = output_channel\n",
    "    :param first_block: if this is the first residual block of the whole network\n",
    "    :return: 4D tensor.\n",
    "    '''\n",
    "    input_channel = input_layer.get_shape().as_list()[-1]\n",
    "\n",
    "    # When it's time to \"shrink\" the image size, we use stride = 2\n",
    "    if input_channel * 2 == output_channel:\n",
    "        increase_dim = True\n",
    "        stride = 2\n",
    "    elif input_channel == output_channel:\n",
    "        increase_dim = False\n",
    "        stride = 1\n",
    "    else:\n",
    "        raise ValueError('Output and input channel does not match in residual blocks!!!')\n",
    "\n",
    "    # The first conv layer of the first residual block does not need to be normalized and relu-ed.\n",
    "    with tf.variable_scope('conv1_in_block'):\n",
    "        if first_block:\n",
    "            filter = create_variables(name='conv', shape=[3, 3, input_channel, output_channel])\n",
    "            conv1 = tf.nn.conv2d(input_layer, filter=filter, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        else:\n",
    "            conv1 = bn_relu_conv_layer(input_layer, [3, 3, input_channel, output_channel], stride)\n",
    "\n",
    "    with tf.variable_scope('conv2_in_block'):\n",
    "        conv2 = bn_relu_conv_layer(conv1, [3, 3, output_channel, output_channel], 1)\n",
    "\n",
    "    # When the channels of input layer and conv2 does not match, we add zero pads to increase the\n",
    "    #  depth of input layers\n",
    "    if increase_dim is True:\n",
    "        pooled_input = tf.nn.avg_pool(input_layer, ksize=[1, 2, 2, 1],\n",
    "                                      strides=[1, 2, 2, 1], padding='VALID')\n",
    "        padded_input = tf.pad(pooled_input, [[0, 0], [0, 0], [0, 0], [input_channel // 2,\n",
    "                                                                     input_channel // 2]])\n",
    "    else:\n",
    "        padded_input = input_layer\n",
    "\n",
    "    output = conv2 + padded_input\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(input_tensor_batch, n, reuse):\n",
    "    '''\n",
    "    The main function that defines the ResNet. total layers = 1 + 2n + 2n + 2n +1 = 6n + 2\n",
    "    :param input_tensor_batch: 4D tensor\n",
    "    :param n: num_residual_blocks\n",
    "    :param reuse: To build train graph, reuse=False. To build validation graph and share weights\n",
    "    with train graph, resue=True\n",
    "    :return: last layer in the network. Not softmax-ed\n",
    "    '''\n",
    "\n",
    "    layers = []\n",
    "    with tf.variable_scope('conv0', reuse=reuse):\n",
    "        conv0 = conv_bn_relu_layer(input_tensor_batch, [3, 3, 3, 16], 1)\n",
    "        activation_summary(conv0)\n",
    "        layers.append(conv0)\n",
    "\n",
    "    for i in range(n):\n",
    "        with tf.variable_scope('conv1_%d' %i, reuse=reuse):\n",
    "            if i == 0:\n",
    "                conv1 = residual_block(layers[-1], 16, first_block=True)\n",
    "            else:\n",
    "                conv1 = residual_block(layers[-1], 16)\n",
    "            activation_summary(conv1)\n",
    "            layers.append(conv1)\n",
    "\n",
    "    for i in range(n):\n",
    "        with tf.variable_scope('conv2_%d' %i, reuse=reuse):\n",
    "            conv2 = residual_block(layers[-1], 32)\n",
    "            activation_summary(conv2)\n",
    "            layers.append(conv2)\n",
    "\n",
    "    for i in range(n):\n",
    "        with tf.variable_scope('conv3_%d' %i, reuse=reuse):\n",
    "            conv3 = residual_block(layers[-1], 64)\n",
    "            layers.append(conv3)\n",
    "        assert conv3.get_shape().as_list()[1:] == [8, 8, 64]\n",
    "\n",
    "    with tf.variable_scope('fc', reuse=reuse):\n",
    "        in_channel = layers[-1].get_shape().as_list()[-1]\n",
    "        bn_layer = batch_normalization_layer(layers[-1], in_channel)\n",
    "        relu_layer = tf.nn.relu(bn_layer)\n",
    "        global_pool = tf.reduce_mean(relu_layer, [1, 2])\n",
    "\n",
    "        assert global_pool.get_shape().as_list()[-1:] == [64]\n",
    "        output = output_layer(global_pool, 10)\n",
    "        layers.append(output)\n",
    "\n",
    "    return layers[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_graph(train_dir='logs'):\n",
    "    '''\n",
    "    Run this function to look at the graph structure on tensorboard. A fast way!\n",
    "    :param train_dir:\n",
    "    '''\n",
    "    input_tensor = tf.constant(np.ones([128, 32, 32, 3]), dtype=tf.float32)\n",
    "    result = inference(input_tensor, 2, reuse=False)\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    summary_writer = tf.train.SummaryWriter(train_dir, sess.graph)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
